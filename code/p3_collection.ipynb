{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "I collect posts from the past two years from the Magic, Eternal, and Hearthstone subreddits here. This code is meant to be run a single time and then saves the collected data to a csv file (tcg_raw.csv). This collection will only occur again if the code cannot find a data file to read, due to the length of time required to scrape all of the data from the API. 100 posts are gathered from each subreddit for each week from the past 2 years, resulting in a dataset containing over 30,000 entries. I gathered the data in this way in order to mitigate results pertaining to specific card sets or events, in order to gain more understanding on game-wide trends. The following information is gathered\n",
    "\n",
    "|Feature|Type|Description|\n",
    "|---|---|---|\n",
    "num_comments|int|The number of comments attached to the post\n",
    "title|str|The title of the post\n",
    "sub|int|The subreddit that the post originated from, encoded as integers: magicTCG (0), EternalCardGame (1), or hearthstone (2)\n",
    "\n",
    "It is important to note here that I did *not* take the selftext from each title. The reason I chose to do this is that a large number of posts do not have any selftext associated with them. Those that do have selftext will likely contain words that are already in or related to the title itself, and so would not necessarily add useful information to the models, and so I choose to only analyze the titles themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulls 100 posts per week from each subreddit for the past two years. This notebook should only be run once! It takes a long time to collect all the posts\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "\n",
    "if not pathlib.Path('../datasets/tcg_raw.csv'):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    reddits = ['magicTCG', 'EternalCardGame', 'hearthstone']\n",
    "    subs = []\n",
    "\n",
    "    for tcg in reddits:\n",
    "        params = {'subreddit':tcg,\n",
    "                  'size':100,\n",
    "                  'fields':['title', 'num_comments']}\n",
    "\n",
    "        for days in range(104):\n",
    "            params['before'] = str(7*days) + 'd'\n",
    "            params['after'] = str(7*(days + 1)) + 'd'\n",
    "            data = requests.get(url, params).json()\n",
    "            posts = data['data']\n",
    "            for post in posts:\n",
    "                post['sub'] = tcg\n",
    "            subs += [post for post in posts]\n",
    "\n",
    "            time.sleep(3)\n",
    "            print(f'Scraped {len(posts)} posts from {days} weeks ago from {tcg}')\n",
    "\n",
    "    df = pd.DataFrame(subs)\n",
    "    df['sub'] = df['sub'].map({'magicTCG':0, 'EternalCardGame':1,'hearthstone':2})\n",
    "    df.drop_duplicates()\n",
    "    df.to_csv('../datasets/tcg_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Card Lists\n",
    "\n",
    "I also load the full card lists (or, due to Magic's long history, only the past 2 year's worth of cards) in order to add these words as stop words. This is to reduce our model's reliance on specific card names or proper nouns found only in one specific game. Creates a single list of all card names and stores that list to be passed to the EDA notebook for parsing and duplicate removal. Sources for the card lists can be found in the project summary document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cards = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for card in pd.read_csv('../datasets/mtg_cards.csv')['Card']:\n",
    "    all_cards += card + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for card in pd.read_csv('../datasets/hearthstone_cards.csv')['Card']:\n",
    "    all_cards += card + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for card in pd.read_csv('../datasets/eternal_cards.csv')['Cards']:\n",
    "    all_cards += card + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%store all_cards"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
